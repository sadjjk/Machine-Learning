# C4.5-剪枝处理

#### 背景

决策树在学习的过程中 不停地对节点进行划分

因此必然会导致整棵树的分支过多 分的太小细

进而造成了**过拟合**

所以需要对整棵决策树进行**剪枝**

这也是树结构独有的优化方式

#### 概述

剪枝策略有两种：预剪枝和后剪枝

**预剪枝**: 在构造决策树的过程中，先对每个节点在划分前进行估计，若果当前节点的划分不能带来模型泛化性能的提升，则不对当前节点进行划分

**后剪枝**: 在整棵决策树构造完毕后，自底向上地对各个节点继续判断，若将该节点不能带来模型泛化性能的提升，则舍弃当前节点

#### 理论

模型泛化性能的提升 主要的靠验证集的数据来验证

若当前节点 划分前验证集的精确率 >= 划分后 验证集的精确率

则没有带来性能提升 即不需要当前节点的划分

这是一个未经剪枝的决策树

![img](https://i.loli.net/2021/09/18/YLes4rizujvmNPM.jpg)

##### 预剪枝

经过预剪枝后 仅剩下一个节点 

![img](https://i.loli.net/2021/09/18/PojAJhk19aWrvuG.jpg)

**优点:**

- 预剪枝使得决策树的很多节点都没有展开，降低了过拟合的风险
- 同时还减少了决策树的训练/测试的内存开销和时间开销

**缺点:**

虽然当前划分不能提升泛化性能，但是基于该划分的后续划分却有可能导致性能提升

因此预剪枝决策树有可能带来欠拟合的风险

##### 后剪枝

经过后剪枝 保留了较多的节点

![img](https://i.loli.net/2021/09/18/mO1Jq7wry3plKkY.jpg)

**优点:**

一般情形下，后剪枝决策树的欠拟合风险小，泛化性能往往也要优于预剪枝

**缺点:**

不仅要构造完整的决策树 还要自底向上的判断各个节点 增加了内存开销和时间开销

##### 总结

出于模型效果的要求 常使用**后剪枝**优化决策树 来提高模型的泛化性 

