# ID3理论概述

#### 概述

ID3算法作为最早提出的决策树算法 可以说是奠定了树类算法的最重要的基础

后续的算法都是在此基础上进行修修补补 一步步发展成了现在的Xgboost、LightGBM

从现在的眼光去看ID3 可以说是各种问题 啥也不是 比如：

1. 只能预测分类问题 ，无法预测回归问题
2. 特征只能离散值 不能是连续值 也不能有缺失值
3. 没有剪枝 很容易过拟合

但在1979年(30年前)可以说突破性成果

#### 基本概念

决策树 首先一定是一棵树，树包含一个根节点、若干个分支结点和若干个叶结点

每个分支结点代表一个case ，每个叶结点代表一个case的输出结果

决策树 就是将一系列带有特征和标签的数据转化成一个棵带有标签输出的树

以西瓜数据集为例

样例数据
|编号|色泽|根蒂|敲声|纹理|脐部|触感|类别(label)|
|--|--|--|--|--|--|--|--|
|1|青绿|蜷缩|浊响|清晰|凹陷|硬滑|好瓜|
|2|乌黑|蜷缩|沉闷|清晰|凹陷|硬滑|好瓜|
|3|乌黑|蜷缩|浊响|清晰|凹陷|硬滑|好瓜|
|4|青绿|蜷缩|沉闷|清晰|凹陷|硬滑|好瓜|
|5|浅白|蜷缩|浊响|清晰|凹陷|硬滑|好瓜|
|6|青绿|稍蜷|浊响|清晰|稍凹|软粘|好瓜|
|7|乌黑|稍蜷|浊响|稍糊|稍凹|软粘|好瓜|
|8|乌黑|稍蜷|浊响|清晰|稍凹|硬滑|好瓜|
|9|乌黑|稍蜷|沉闷|稍糊|稍凹|硬滑|坏瓜|
|10|青绿|硬挺|清脆|清晰|平坦|软粘|坏瓜|
|11|浅白|硬挺|清脆|模糊|平坦|硬滑|坏瓜|
|12|浅白|蜷缩|浊响|模糊|平坦|软粘|坏瓜|
|13|青绿|稍蜷|浊响|稍糊|凹陷|硬滑|坏瓜|
|14|浅白|稍蜷|沉闷|稍糊|凹陷|硬滑|坏瓜|
|15|乌黑|稍蜷|浊响|清晰|稍凹|软粘|坏瓜|
|16|浅白|蜷缩|浊响|模糊|平坦|硬滑|坏瓜|
|17|青绿|蜷缩|沉闷|稍糊|稍凹|硬滑|坏瓜|

使用ID3算法后得出的决策树

<img src="https://i.loli.net/2021/09/14/v1XUTKqbSVMg4Ye.png" alt="img" style="zoom:50%;" />
当有新的数据产生要时 判断该瓜是好瓜还是坏瓜

则可按照该构造的决策树进行依次判断

先判断纹理是清晰/稍糊/模糊? 在判断根蒂/触感 ...

从而实现了预测的目的

#### 关键点

决策树的关键在于如何选择最优的划分点，尽量使划分的样本属于同一类别？

即划分后的样本尽可能纯

那么如何来度量纯度？

ID3使用**信息熵**来度量纯度，使用**信息增益**来度量划分前与划分后的纯度差异

*C4.5、CART最核心的优化点也都是在这个地方*

当按照**某个属性**划分时 该划分前与划分后的纯度差异达到最大时 则为最优划分点

##### 信息熵

假如当前样本集D中第k类样本所占的比例为![img](https://img-blog.csdn.net/20180129135013432)，![img](https://img-blog.csdn.net/20180129135213195) 为类别的总数即label的不重复数（对于二元分类来说，![img](https://img-blog.csdn.net/20180129135237279)）。则样本集的信息熵为

![img](https://img-blog.csdn.net/20180129135548693)

##### 信息增益

假定离散属性 ![img](https://img-blog.csdn.net/20180129140439073) 有 ![img](https://img-blog.csdn.net/20180129140514900)个可能的取值![img](https://img-blog.csdn.net/20180129141123940)，如果使用特征 ![img](https://img-blog.csdn.net/20180129140439073) 来对数据集D进行划分，则会产生V个分支结点， 其中第v个结点包含了数据集D中所有在特征 ![img](https://img-blog.csdn.net/20180129140439073) 上取值为 ![img](https://img-blog.csdn.net/20180129141756607)的样本总数，记为![img](https://img-blog.csdn.net/20180129141835120)。

![img](https://img-blog.csdn.net/20180129161423421)

#### 算法步骤

以上述西瓜集为例 

1.计算数据集的信息熵

该数据集包含17个样本，label为好瓜与坏瓜，即![img](https://img-blog.csdn.net/20180129135237279)。

则 正例（类别为好瓜的样本）占的比例为：![img](https://img-blog.csdn.net/20180129161637039)，反例（类别为坏瓜的样本）占的比例为：![img](https://img-blog.csdn.net/20180129161649348)

该数据的信息熵为
$$
\begin{eqnarray}
Ent(D) & = & - \sum_{k=1}^{K}p_{k}log_2p_{k} \\
      ~& = & - (\frac{8}{17} \times log_{2}\frac{8}{17} +  \frac{9}{17} \times log_{2}\frac{9}{17}) \\
      ~& = & 0.9975
\end{eqnarray}
$$
2.计算各个特征的信息熵

特征分别为：色泽、根蒂、敲声、纹理、脐部、触感 

计算每个特征的信息增益
$$
\begin{eqnarray} 
Ent(D_{色泽}^{浅白}) &= & - (\frac{1}{5} \times log_{2}\frac{1}{5} + \frac{4}{5} \times log_{2}\frac{4}{5}) = 0.7219 \\
Ent(D_{色泽}^{青绿}) & =&  - (\frac{3}{6} \times log_{2}\frac{3}{6} + \frac{3}{6} \times log_{2}\frac{3}{6}) = 1 \\
Ent(D_{色泽}^{乌黑}) &= & - (\frac{2}{6} \times log_{2}\frac{2}{6} + \frac{4}{6} \times log_{2}\frac{4}{6}) = 0.9183 \\
Gain(D,色泽) &=& Ent(D) - (\frac{5}{17} \times Ent(D_{色泽}^{浅白}) + \frac{6}{17} \times Ent(D_{色泽}^{青绿}) + \frac{6}{17} \times Ent(D_{色泽}^{乌黑})) \\
 ~&= &0.9975 - (\frac{5}{17} \times  0.7219 + \frac{6}{17} \times 1+\frac{6}{17} \times 0.9183 ) = 0.1081 \\
Ent(D_{根蒂}^{蜷缩}) &=&  - (\frac{5}{8} \times log_{2}\frac{5}{8} + \frac{3}{8} \times log_{2}\frac{3}{8}) = 0.9544\\
Ent(D_{根蒂}^{稍蜷}) &=&  - (\frac{3}{7} \times log_{2}\frac{3}{7} + \frac{4}{7} \times log_{2}\frac{4}{7}) = 0.9852\\
Ent(D_{根蒂}^{硬挺}) &=&  - (\frac{0}{2} \times log_{2}\frac{0}{2} + \frac{2}{2} \times log_{2}\frac{2}{2}) = 0\\
Gain(D,根蒂) &=& Ent(D) - (\frac{8}{17} \times Ent(D_{根蒂}^{蜷缩}) + \frac{5}{17} \times Ent(D_{根蒂}^{稍蜷}) + \frac{2}{17} \times Ent(D_{根蒂}^{硬挺})) \\
 ~&=& 0.9975 - (\frac{8}{17} \times  0.9544 + \frac{7}{17} \times 0.9852 +\frac{2}{17} \times 0 ) = 0.1427 \\
Ent(D_{敲声}^{沉闷}) &=&  - (\frac{2}{5} \times log_{2}\frac{2}{5} + \frac{3}{5} \times log_{2}\frac{3}{5}) = 0.9710\\
Ent(D_{敲声}^{清脆}) &=&  - (\frac{0}{2} \times log_{2}\frac{0}{2} + \frac{2}{2} \times log_{2}\frac{2}{2}) = 0\\
Ent(D_{敲声}^{浊响}) &=&  - (\frac{6}{10} \times log_{2}\frac{6}{10} + \frac{4}{10} \times log_{2}\frac{4}{10}) = 0.9710\\
Gain(D,敲声) &=& Ent(D) - (\frac{8}{17} \times Ent(D_{敲声}^{沉闷}) + \frac{5}{17} \times Ent(D_{敲声}^{清脆}) + \frac{2}{17} \times Ent(D_{敲声}^{浊响})) \\
 ~&=& 0.9975 - (\frac{5}{17} \times  0.9710 + \frac{2}{17} \times 0 +\frac{10}{17} \times 0.9710 ) = 0.1407 \\
Ent(D_{纹理}^{模糊}) &=&  - (\frac{3}{3} \times log_{2}\frac{3}{3} + \frac{0}{3} \times log_{2}\frac{0}{3}) = 0\\
Ent(D_{纹理}^{清晰}) &=&  - (\frac{2}{9} \times log_{2}\frac{2}{9} + \frac{7}{9} \times log_{2}\frac{7}{9}) = 0.7642\\
Ent(D_{纹理}^{稍糊}) &=&  - (\frac{1}{5} \times log_{2}\frac{1}{5} + \frac{4}{5} \times log_{2}\frac{4}{5}) = 0.7219\\
Gain(D,{纹理}) &=& Ent(D) - (\frac{3}{17} \times Ent(D_{纹理}^{模糊}) + \frac{9}{17} \times Ent(D_{纹理}^{清晰}) + \frac{5}{17} \times Ent(D_{纹理}^{稍糊})) \\
 ~&=& 0.9975 - (\frac{3}{17} \times  0 + \frac{9}{17} \times 0.7642 +\frac{5}{17} \times 0.7219 ) = 0.3806 \\
Ent(D_{脐部}^{凹陷}) &=&  - (\frac{2}{7} \times log_{2}\frac{2}{7} + \frac{5}{7} \times log_{2}\frac{5}{7}) = 0.8631\\
Ent(D_{脐部}^{平坦}) &=&  - (\frac{0}{4} \times log_{2}\frac{0}{4} + \frac{4}{4} \times log_{2}\frac{4}{4}) = 0\\
Ent(D_{脐部}^{稍糊}) &=&  - (\frac{3}{6} \times log_{2}\frac{3}{6} + \frac{3}{6} \times log_{2}\frac{3}{6}) = 1\\
Gain(D,{脐部}) &=& Ent(D) - (\frac{7}{17} \times Ent(D_{脐部}^{凹陷}) + \frac{4}{17} \times Ent(D_{脐部}^{平坦}) + \frac{6}{17} \times Ent(D_{脐部}^{稍糊})) \\
 ~&=& 0.9975 - (\frac{7}{17} \times  0.8631 + \frac{4}{17} \times 0 +\frac{6}{17} \times 1 ) = 0.2892 \\
Ent(D_{触感}^{软粘})  &=&  - (\frac{2}{5} \times log_{2}\frac{2}{5} + \frac{3}{5} \times log_{2}\frac{3}{5}) = 0.9710\\
Ent(D_{触感}^{硬滑}) &=&  - (\frac{6}{12} \times log_{2}\frac{6}{12} + \frac{6}{12} \times log_{2}\frac{6}{12}) = 1\\
Gain(D,触感)  &=& Ent(D) - (\frac{5}{17} \times Ent(D_{触感}^{软粘}) + \frac{12}{17} \times Ent(D_{触感}^{硬滑}) \\
 ~&=& 0.9975 - (\frac{5}{17} \times  0.9710 + \frac{12}{17} \times 1 ) = 0.0060 \\
feature&=&argmax(Gain(D,色泽),Gain(D,根蒂),Gain(D,敲声),Gain(D,{纹理}),Gain(D,{脐部}),Gain(D,触感) )\\
~&=&argmax(0.1081,0.1427,0.1407, 0.3806,0.2892,0.0060 ) \\
~&=&纹理

\end{eqnarray}
$$
选择信息增益最大的特征:纹理

因此划分为

 <img src="https://i.loli.net/2021/09/14/zJSpLGy6nbQEIfu.png" alt="image-20210311165331682" style="zoom:50%;" />

3.继续各分支进行划分

- **分支结点:纹理=清晰**
  $$
  \begin{eqnarray} 
  Gain(D_{纹理=清晰},色泽)  &=&  0.0431 \\
  Gain(D_{纹理=清晰},根蒂)  &=&  0.4581 \\
  Gain(D_{纹理=清晰},脐部)  &=&  0.4581 \\
  Gain(D_{纹理=清晰},触感)  &=&  0.4581 \\
  Gain(D_{纹理=清晰},敲声)  &=&  0.3309 \\
  \end{eqnarray}
  $$
  “根蒂”、“脐部”、“触感”这3个属性均取得了最大的信息增益，

  可以随机选择其中之一作为划分属性 比如选择“根蒂”

  划分为

  <img src="https://i.loli.net/2021/09/14/Ti3e6xsIpVRjmvQ.png" alt="image-20210311180027262" style="zoom:50%;" />

  - 继续对 **分支结点:纹理=清晰 & 根蒂=蜷缩** 进行划分

    观察样本集 编号为1,2,3,4,5的样本均为好瓜 当前节点标记为叶子节点 无需再划分

    <img src="https://i.loli.net/2021/09/14/YnfiDUy9T4gxCNM.png" alt="img" style="zoom:50%;" />

  - 继续对 **分支结点:纹理=清晰 & 根蒂=稍蜷** 进行划分
    $$
    \begin{eqnarray} 
    Gain(D_{纹理=清晰 \& 根蒂=稍蜷 },色泽)  &=&  0.2516 \\
    Gain(D_{纹理=清晰 \& 根蒂=稍蜷 },触感)  &=&  0.2516 \\
    Gain(D_{纹理=清晰 \& 根蒂=稍蜷 },敲声)  &=&  0 \\
    Gain(D_{纹理=清晰 \& 根蒂=稍蜷 },脐部)  &=&  0 \\
    \end{eqnarray}
    $$
    “色泽”、“触感”这2个属性均取得了最大的信息增益，

    可以随机选择其中之一作为划分属性 比如选择“色泽”

    <img src="https://i.loli.net/2021/09/14/pzRsZk4SE8yBLGU.png" alt="img" style="zoom:50%;" />

    - **”纹理=清晰 & 根蒂=稍蜷 & 色泽=青绿“** 

      只包含一个样本: 6号 类别标签为好瓜  当前节点标记为叶子节点无需再划分 

    - **”纹理=清晰 & 根蒂=稍蜷 & 色泽=浅白“ 为空集 不能划分** 对应的措施为:

      将其设置为叶子节点 类别设置为其父节点中所含样本最多的类别 即

      6号为好瓜、8号为好瓜、15号为坏瓜 2个好瓜 1个坏瓜

      故对应类别是 好瓜

    - **”纹理=清晰 & 根蒂=稍蜷 & 色泽=乌黑“ ** 

      继续进行划分 
      $$
      \begin{eqnarray} 
      Gain(D_{纹理=清晰 \& 根蒂=稍蜷 \& 色泽=乌黑},触感)  &=&  1 \\
      Gain(D_{纹理=清晰 \& 根蒂=稍蜷 \& 色泽=乌黑},脐部)  &=&  0 \\
      Gain(D_{纹理=清晰 \& 根蒂=稍蜷 \& 色泽=乌黑},敲声)  &=&  0 \\
      \end{eqnarray}
      $$
      “触感” 取得了最大的信息增益，按”触感“进行划分

    得到如下

    <img src="https://i.loli.net/2021/09/14/hvBzT7kp5QoWt63.png" alt="img" style="zoom:50%;" />

  - 继续对 **分支结点:纹理=清晰 & 根蒂=硬挺** 进行划分

    只包含一个样本: 10号 类别标签为坏瓜  当前节点标记为叶子节点无需再划分

  得到如下

  <img src="https://i.loli.net/2021/09/14/qyMYcU6EPRWgF5o.png" alt="img" style="zoom:50%;" />

- 对**分支结点:纹理=稍糊** 进行划分
  $$
  \begin{eqnarray} 
  Gain(D_{纹理=稍糊 },色泽)  &=&  0.3219 \\
  Gain(D_{纹理=稍糊 },根蒂)  &=&  0.07291 \\
  Gain(D_{纹理=稍糊 },敲声)  &=&  0.3219 \\
  Gain(D_{纹理=稍糊 },脐部)  &=&  0.171 \\
  Gain(D_{纹理=稍糊 },触感)  &=&  0.7219 \\
  \end{eqnarray}
  $$
   选择最大信息增益: 触感 进行划分 如下:

   <img src="https://i.loli.net/2021/09/14/5c7nGTIVaHFqlvw.png" alt="img" style="zoom:50%;" />

  - **”纹理=稍糊 & 根触感=软粘“**

     只包含一个样本: 7号 类别标签为好瓜  当前节点标记为叶子节点无需再划分 

  - **”纹理=稍糊 & 根触感=硬滑“**

    该分支结点下4个样本的标签均为坏瓜 故当前节点标记为叶子节点无需再划分 

  得到如下

  <img src="https://i.loli.net/2021/09/14/961cJngidFQTz8L.png" alt="img" style="zoom:50%;" />

- 对**分支结点:纹理=模糊** 进行划分

  该分支结点下3个样本的标签均为坏瓜 故当前节点标记为叶子节点无需再划分

最后得到如下

<img src="https://i.loli.net/2021/09/14/U6yNJXupCeZATmQ.png" alt="img" style="zoom:50%;" />

#### 总结

ID3算法提出的时间较早 当时仅仅是为了解决特定问题

因此缺点也很明显

1. 只能预测分类问题 ，无法预测回归问题

2. 特征只能离散值 不能是连续值 也不能有缺失值

   *这个在举例数据时可以发现 所有的特征都是离散值 没有连续值*

   *在当初设计的时候就没有将连续值考虑进去* 

3. 没有剪枝 很容易过拟合

4. **使用信息增益来衡量纯度时 有一个很严重的缺点**

   当特征的值(不重复数)越多 ，信息增益越大

   即这个决策树一定是选择特征值多的特征进行分支

   举一个极端的例子 依然以西瓜数据集为例

   当把西瓜编号加入特征池时

   那么编号的信息增益一定是最大的 (因为每个编号对应一个西瓜类别 每个编号的熵均为0)

   于是产生17个分支，每个分支对应一个样本 如下

   <img src="https://img-blog.csdn.net/20170508144129153" alt="img" style="zoom: 75%;" />

   这样的决策树不具有任何泛化能力 